\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor} % For cell coloring
\usepackage{geometry} % For page margins
\usepackage{graphicx} % For including images
\usepackage{float} % For image placement
\usepackage{tabularx} % For table stretching
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{amsmath}

\usepackage{listings}


\usepackage{titlesec}
\titleformat{\subparagraph}
    {\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{}
\titlespacing*{\subparagraph}{\parindent}{3.25ex plus 1ex minus .2ex}{.75ex plus .1ex}


\input{/home/oddish3/Documents/uni/mast-2/preamble.tex}
\input{/home/oddish3/Documents/uni/mast-2/notation.tex}
\input{../../results/results.tex}

% Define colours
\definecolor{headercolor}{RGB}{169,169,169} % Grey color, adjust the RGB values as per your document
\definecolor{rowcolor}{RGB}{255,255,255} % White color, adjust if your document has a different row color

% Set page margins
\geometry{left=25mm,right=25mm,top=25mm,bottom=25mm}

% Custom column type for tabularx
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\title{ECON60332 Coursework Template}
\author{Group 11} % INSERT GROUP NUMBER HERE
\date{}

\begin{document}

\maketitle

\noindent\begin{tabularx}{\linewidth}{|Y|Y|}
    \hline
    \rowcolor{headercolor} Participant’s Student ID & Indicate if the student did not participate \\
    \hline
    \multicolumn{1}{|c|}{10710007}\\
    \hline

\end{tabularx}

\section*{Theoretical exercise}

\subsection*{Question a}

Similarities 
\begin{itemize}
	\item Both models account for volatility clustering, where periods of high (low) volatility are followed by periods of high (low) volatility 
		\item Both models include lagged square error terms $\epsilon^2_{t}$ in ARCH(2) and both $\epsilon^2$ and $\sigma^2$ in GARCH(1,1) 
\end{itemize}

Differences
\begin{itemize}
	\item In the GARCH(1,1) model, past squared residuals and past variances both influence the current variance but in the ARCH(2) model, it only considers past squared residuals,  reducing the persistence of shocks
	\item The GARCH(1,1) has a mean-reverting component through the $\sigma^2_{t-1}$ term, which isn't present in the ARCH(2) model 
\end{itemize}

\subsection*{Question b}

The unconditional mean of the error term, $E \left[ \epsilon_t \right]$ is given by
\[
	E \left[ \epsilon_t \right] = E \left[ z_{t}  \right] = E \left[ \sqrt{\sigma_t^2}   \right]   = 0 \cdot E \left[ \sigma_t \right] = 0
\]
Since by definition of $z_t \stackrel{i . i . d .}{\sim} \mathcal{N}(0,1)$, $E \left[ z_{t} \right] =0$ follows a standard normal distribution. 

To derive the unconditional variance, we express $\epsilon_t^2$ as an AR(2) process 

\begin{align*}
\sigma_t^2 + \epsilon_t^2 &= \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon_{t-2}^2 + \epsilon_t^2 \\
\epsilon_t^2 &= \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon_{t-2}^2 + (\epsilon_t^2 - \sigma_t^2) \\
\Rightarrow \epsilon_t^2 &= \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon_{t-2}^2 + \nu_t \hspace{5mm} \text{where} \ \nu_t = \epsilon_t^2 - \sigma_t^2 \ \text{is a white noise process}  
\end{align*}

Thus, we can rewrite 

\begin{align*}
V(\epsilon_t) & \\
E[\epsilon_t] &= E\left[ \frac{\epsilon_t^2}{\sigma_t^2} \right] = 0 \hspace{5mm} \text{given } |\alpha| < 1 \\
V(\epsilon_t) &= E[\epsilon_t^2] \\
&= E[\omega + \alpha \epsilon_{t-1}^2 + \nu_t] \\
&= \omega + \alpha E[\epsilon_{t-1}^2] + E[\nu_t] \\
&= \omega + \alpha E[\epsilon_{t-1}^2] \\
E[\epsilon_{t-1}^2] - \alpha E[\epsilon_{t-1}^2] &= \omega \\
E[\epsilon_{t-1}^2] (1 - \alpha) &= \omega \\
E[\epsilon_{t-1}^2] &= \frac{\omega}{1 - \alpha_1 - \alpha_2}
\end{align*}

Assuming $\alpha_1 + \alpha_2 < 1$ for stationarity

NEEEEEEEEEEEEEEEEEEEEEEEEDS MERGINGGGGGGGGGGGGGGGGGGG

Let \( \mathcal{F}_{t-1} \) be the past history of the process, then the conditional expectation of \( \epsilon_t \) is given by:
\begin{equation}
E(\epsilon_t | \mathcal{F}_{t-1}) = E(\sigma_t V_t | \mathcal{F}_{t-1}) = \sigma_t E(V_t | \mathcal{F}_{t-1}) = 0
\end{equation}
Thus:
\begin{equation}
E(\epsilon_t) = E(E(\epsilon_t | \mathcal{F}_{t-1})) = E(0) = 0
\end{equation}

To derive the second moment, note that you can express \( \epsilon_t^2 \) as an ARMA(1,1)-process:
\begin{equation}
\epsilon_t^2 = \alpha_0 + (\alpha_1 + \beta_1)\epsilon_{t-1}^2 + w_t - \beta_1 w_{t-1}
\end{equation}
with \( w_t = \epsilon_t^2 - \sigma_t^2 \) being a WN process.

It holds that:
\begin{equation}
E(\epsilon_t^2 | \mathcal{F}_{t-1}) = E(\sigma_t^2 V_t^2 | \mathcal{F}_{t-1}) = \sigma_t^2 E(V_t^2 | \mathcal{F}_{t-1}) = \sigma_t^2 \cdot 1 = \sigma_t^2
\end{equation}
Thus:
\begin{equation}
E(w_t | \mathcal{F}_{t-1}) = E(\epsilon_t^2 | \mathcal{F}_{t-1}) - \sigma_t^2 = \sigma_t^2 - \sigma_t^2 = 0
\end{equation}
Therefore:
\begin{equation}
E(w_t) = E(E(w_t | \mathcal{F}_{t-1})) = E(0) = 0
\end{equation}

This allows us to write:
\begin{equation}
E(\epsilon_t^2) = \alpha_0 + (\alpha_1 + \beta_1)E(\epsilon_{t-1}^2) + E(w_t) - \beta_1 E(w_{t-1}) = \alpha_0 + (\alpha_1 + \beta_1)E(\epsilon_{t-1}^2)
\end{equation}
Assuming stationarity, we conclude that \( E(\epsilon_{t-1}^2) = E(\epsilon_t^2) \) and hence:
\begin{equation}
\mathrm{Var}(\epsilon_t) = E(\epsilon_t^2) - (E(\epsilon_t))^2 = E(\epsilon_t^2) = \frac{\alpha_0}{1 - (\alpha_1 + \beta_1)}
\end{equation}
If we impose the restriction \( \alpha_1 + \beta_1 < 1 \), \( \mathrm{Var}(\epsilon_t) \) exists and is finite.

Now, observe that:
\begin{equation}
\mathrm{Cov}(\epsilon_t, \epsilon_{t-\tau}) = E(\epsilon_t \epsilon_{t-\tau}) - E(\epsilon_t)E(\epsilon_{t-\tau}) = E(\epsilon_t \epsilon_{t-\tau}) - E(\epsilon_t | \mathcal{F}_{t-1})E(\epsilon_{t-\tau} | \mathcal{F}_{t-1}) = 0
\end{equation}



\subsection*{Question c}

Using $\text{Cov} (X, Y) = E[XY] - E[X]E[Y]$ we derive the covariance, where $\sigma^2_{t+1} = \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon^2_{t-2}) \epsilon_t$

\begin{align*}
\text{Cov}(\sigma^2_{t+1}, \epsilon_t | F_{t-1}) &= E[ \sigma^2_{t+1} \epsilon_t | F_{t-1}] - E[\sigma^2_{t+1}| F_{t-1} ] E[\epsilon_{t-1} | F_{t-1})
\end{align*}

Given that $\epsilon_t$ has a conditional expectation of 0, the second term becomes 0, leaving

\begin{align*}
E[ \sigma^2_{t+1} \epsilon_t | \ensuremath{\mathcal{F}}_{t-1}] = E \left[ \left( \omega + \alpha_1 \epsilon_t^2 + \alpha_2 \epsilon^2_{t-1} \right) \epsilon_t^2 \right] \\
&= E \left[  \omega \epsilon_t + \alpha_1 \epsilon_t^3 + \alpha_2 \epsilon^2_{t-1}  \epsilon_t  \right]
\end{align*}

Then,  since $\omega$ is a constant and the conditional mean of $\epsilon_t =0$ (previously shown), the first term disappears. 

Then, simplifying further, since $\sigma^2_{t}$ is $\ensuremath{\mathcal{F}}_{t-1}$deterministic 
We can use the property $E[XY \mid \mathcal{F}_{t-1}] = E[X \mid \mathcal{F}_{t-1}]E[Y \mid \mathcal{F}_{t-1}]$ for independent random variables $X$ and $Y$:

\begin{equation}
E[\alpha_2 \sigma_t^2 \epsilon_t \mid \mathcal{F}_{t-1}] = \alpha_2 \sigma_t^2 E[\epsilon_t \mid \mathcal{F}_{t-1}] = 0
\end{equation}

Therefore, we obtain:

\begin{equation}
\alpha_1 E[\epsilon_t^3 \mid \mathcal{F}_{t-1}]
\end{equation}

Which we can rewrite as
 \[
 a_1 E \left[ zt^3 \right] E \left[ ( \omega + \alpha_1 \epsilon_t^2 + \alpha_2 \epsilon^2_{t-1} )^\frac{3}{2} \mid \ensuremath{\mathcal{F}}_{t-1}  \right] 
 \] 
 \[
 = \alpha_1 \left( \omega + \alpha_1 \epsilon_t^2 + \alpha_2 \epsilon^2_{t-1} \right) )^{\frac{3}{2}} E \left[ z_{t}^3 \right]  
 \] 
35:52 GARCH
NEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEDDS CHECKING - WATCH OVER VID

If $z_t$ follows a symmetric distribution like the standard normal, the third moment is 0. 
The leverage effect refers to the tendency for negative innovations (negative returns) to be associated with higher future volatility, and positive innovations (positive returns) to be associated with lower future volatility.

if the standardized innovation z_t has a symmetric distribution, meaning its third moment E[z_t^3] is zero, then the covariance between ε_t and σ^2_(t+1) will be zero. This implies that the model cannot capture the leverage effect, which is the tendency for negative shocks to have a different impact on volatility than positive shocks.

\subsection*{Question d}

With our ARCH(2) model, the 1 step ahead forecast is : $\sigma^2_{t+1 | t}  = \omega + \alpha_1  \epsilon^2_{t}  + \alpha_2  \epsilon^2_{t-1}$

Analogously the 2 step ahead forecast, where we take expectation of $\epsilon_t^2$, and use the unit variance of $z_{t}$ to simplify 
\begin{align*}
\sigma^2_{t+2 | t}  = E_t \left[  \omega + \alpha_1  \epsilon^2_{t+1}  + \alpha_2  \epsilon^2_{t}  \right] =   \omega + \alpha_1  \sigma^2_{t+1|t}  + \alpha_2  \sigma^2_{t}  
\end{align*}

Similarly for the 3-step ahead forecast, following the same logic

 \begin{align*}
 	\sigma^2_{t+3 | t} = E_t \left[  \omega + \alpha_1  \epsilon^2_{t+2}  + \alpha_2  \epsilon^2_{t+1}  \right] =   \omega + \alpha_1  \sigma^2_{t+2|t}  + \alpha_2  \sigma^2_{t+1} \equiv E \left[ \sigma^2_{t+3} | F_{t} \right] 
 \end{align*}

\subsection*{Question e}


For an ARCH(2) model, the conditional variance is given by:
\begin{equation}
\sigma_t^2 = \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon_{t-2}^2,
\end{equation}
where \( \sigma_t^2 \) is the conditional variance, \( \epsilon_t \) is the residual at time t, and \( \omega, \alpha_1, \alpha_2 \) are non-negative parameters.

The news impact curve (NIC) equation incorporates asymmetric effects and can be expressed as:
\begin{equation}
\sigma_t^2 = \omega + \alpha_1 (|\epsilon_{t-1}| + \gamma_1 \epsilon_{t-1})^2 + \alpha_2 (|\epsilon_{t-2}| + \gamma_2 \epsilon_{t-2})^2,
\end{equation}
with leverage parameters \( \gamma_1 \) and \( \gamma_2 \). If \( \gamma_1 = \gamma_2 = 0 \), the NIC is symmetric. Asymmetric effects are introduced when \( \gamma_1 \neq 0 \) or \( \gamma_2 \neq 0 \), indicative of a leverage effect.

Expanding the equation, we have:
\begin{equation}
\sigma_t^2 = \omega + \alpha_1 (\epsilon_{t-1}^2 + 2\gamma_1 |\epsilon_{t-1}|\epsilon_{t-1} + \gamma_1^2 \epsilon_{t-1}^2) + \alpha_2 (\epsilon_{t-2}^2 + 2\gamma_2 |\epsilon_{t-2}|\epsilon_{t-2} + \gamma_2^2 \epsilon_{t-2}^2),
\end{equation}
demonstrating the dependence of \( \sigma_t^2 \) on \( \epsilon_{t-1} \) and \( \epsilon_{t-2} \), and the asymmetry introduced by \( \gamma_1 \) and \( \gamma_2 \).
\begin{equation}
\text{NIC}(\epsilon_{t-1} | \sigma_{t-1}^2 = \sigma_Y^2) = \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \sigma_Y^2.
\end{equation}

AAAAAAAAAAAASSSSYMMMETRIC ARCH? 
 not sure

For an ARCH(2) model, the conditional variance is typically given by:
\begin{equation}
\sigma_t^2 = \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon_{t-2}^2,
\end{equation}
where \( \sigma_t^2 \) is the conditional variance, \( \epsilon_t \) is the residual at time t, and \( \omega, \alpha_1, \alpha_2 \) are non-negative parameters.

The news impact curve (NIC) extends this model to account for asymmetric effects, which is especially relevant in financial data where 'bad' and 'good' news might have different impacts on volatility. This is captured by introducing leverage parameters \( \gamma_1 \) and \( \gamma_2 \), modifying the model as follows:
\begin{equation}
\sigma_t^2 = \omega + \alpha_1 (|\epsilon_{t-1}| + \gamma_1 \epsilon_{t-1})^2 + \alpha_2 (|\epsilon_{t-2}| + \gamma_2 \epsilon_{t-2})^2,
\end{equation}
where \( \gamma_1 \) and \( \gamma_2 \) represent the additional impact of negative shocks to volatility compared to positive ones. 

Expanding the NIC equation, we have:
\begin{equation}
\sigma_t^2 = \omega + \alpha_1 (\epsilon_{t-1}^2 + 2\gamma_1 |\epsilon_{t-1}|\epsilon_{t-1} + \gamma_1^2 \epsilon_{t-1}^2) + \alpha_2 (\epsilon_{t-2}^2 + 2\gamma_2 |\epsilon_{t-2}|\epsilon_{t-2} + \gamma_2^2 \epsilon_{t-2}^2),
\end{equation}
which illustrates how conditional variance depends on both the magnitude and sign of the previous shocks, with \(\gamma_1\) and \(\gamma_2\) dictating the asymmetry of the impact.

To visualize the news impact curve for the most recent shock \(\epsilon_{t-1}\), while considering the conditional variances at their long-term average (unconditional variance \(\sigma_Y^2\)), we use the equation:
\begin{equation}
\text{NIC}(\epsilon_{t-1} | \sigma_{t-1}^2 = \sigma_Y^2) = \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \sigma_Y^2.
\end{equation}

The NIC is typically plotted with \(\epsilon_{t-1}\) on the x-axis and \(\sigma_t^2\) on the y-axis, showing the relationship between the magnitude of the shock and the resulting volatility. The asymmetry will be evident if the curve is not symmetric around zero, indicating that the model predicts different levels of volatility for positive and negative shocks of the same magnitude.

\section*{Practical exercise}
\subsection*{Question a}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\rowcolor{headercolor}
 & Sample moments & & Test statistic & P-value \\
\hline
Mean & \amu &   Mean & \amut & \amup \\
\hline
Standard deviation & \asigma &  Skewness & \askewt & \askewp \\
\hline
Skewness &  \askew & Kurtosis & \akurtt & \akurtp \\
\hline
Kurtosis & \akurt & JB & \ajbt & \ajbp \\
\hline
\end{tabular}
\end{table}

\subsection*{LBQ test results:}
\noindent\begin{tabularx}{\linewidth}{|Y|Y|Y|}
    \hline
    \rowcolor{headercolor} & Returns & Squared returns \\
    \hline
    Test statistic & \aistat & \aiistat \\
    \hline
    P-value & \aip & \aiip \\
    \hline
\end{tabularx}

\subsubsection*{Plot of Daily Log Returns}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{../../docs/figures/log_return_plot.png}
    \caption{Log Return Plot}
    \label{fig:logreturn}
\end{figure}

\subsubsection*{SACF and SPACF}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{../../docs/figures/PACF.png}
    \caption{Sample Autocorrelation and Partial Autocorrelation Function Plot}
    \label{fig:pacf}
\end{figure}

\subsection*{Interpretation}

\subparagraph{Descriptive Statistics}  

Mean : the sample mean is \amu 

Standard Deviation : the standard deviation is \asigma, indicating a relatively high volatility of the data. 

Skewness : the skewness value of \askew suggests that the distribution of returns is slightly negatively skewed, meaning there are more extreme negative returns than positive returns. 

Kurtosis : the kurtosis value of \akurt is higher than the normal distribution value of 3, indicating that the distribution of returns has heavier tails and more extreme values when compared to a normal distribution. 

\subparagraph{Test Statistics}

Testing the significance of the mean return, we set up the following hypothesis and use a one sided test :
\begin{itemize}
	\item The null hypothesis, the mean return is equal to zero ($H_0 : \mu =0$) against
	\item The alternative hypothesis that the mean is not equal to zero $H_1 : \mu \neq = 0$.
\end{itemize}
The decision rule is that we reject $H_0$ if the p-value is less than the significance level $\alpha$ (EG, 0.05). 
The test statistic obtained is \amut with a p-value of \amup, thus we fail to reject $H_0$ at all conventional significance levels. 
We do not have enough evidence to conclude the mean return is significantly different from zero, and cannot reject the possibility that the true mean return is equal to zero. 

Testing the significance of skewness in the returns, we set up the following hypothesis and one sided test :
\begin{itemize}
	\item The null hypothesis, the skewness of the return series is zero ($ H_1: \gamma = 0$), indicating a symmetric distribution. Against
	\item The alternative hypothesis, the skewness of the financial time series returns is not zero ($\gamma \neq = 0$), indicating a non-symmetric distribution
\end{itemize}
The decision rule is to reject $H_0$ if the p value is less than the significance level $\alpha$ (EG, 0.05). 
The test statistic obtained is \askewt with a p-value of \askewp. Since the p-value is less than 0.05, we reject the null hypothesis ($H_0$) in favour of the alternative hypothesis. 
Thus, the distribution of returns is significantly skewed, and hence not symmetric. 
Testing for kurtosis (leptokurtic property) in the returns, we set up the following hypothesis and one sided test:
\begin{itemize}
	\item The null hypothesis, the kurtosis of the returns equals the normal distribution value of 3 ($H_0 : \kappa = 3$), indicating a normal distribution in terms of tail thickness. Against, 
	\item The alternative hypothesis, the kurtosis of the financial time series returns is significantly different from 3 ($H_1 : \kappa \neq 3 $), indicating a non-normal distribution in terms of tail thickness
\end{itemize}
The decision rule is to reject $H_0$ if the p value is less than the significance level $\alpha$ (EG, 0.05). 
The test statistic obtained is \akurt and p value of $\akurtp$, thus we reject  $H_0$, in favour of the alternative hypothesis.
Thus, the returns have significantly different kurtosis from 3, confirming the presence of heavy tails in the distribution. 

Using the Jargue-Bera test for normality, we set up the following hypothesis and 1 sided test : 
\begin{itemize}
	\item The null hypothesis, the returns follow a normal distribution, implying that both the skewness and kurtosis of the series equal those of a normal distribution ($H_0 : \gamma = 0 \& \kappa = 3$). Against, 
	\item The alternative hypothesis, that the returns do not follow a normal distribution, meaning either the or both the skewness and kurtosis significantly differ from those of a normal distribution ($H_0 : \gamma \neq 0 \& \kappa \neq 3$). 
\end{itemize}
The decision rule is to reject $H_0$ if the p-value is less than the significance level $\alpha$ (EG, 0.05). 
The outcome of the test since the JB test statistic is \ajbt with a p-value of \ajbp. 
Therefore, we reject $H_0$ in favour of $H_1$, that the distribution of returns differs significantly from normality, evidence by its skewness and kurtosis values. 

\subparagraph{Ljung-Box Test}

Testing the autocorrelation of the financial time series returns, we use the Ljung-Box Q-test with the following hypotheses:

\begin{itemize}
	\item The null hypothesis, which states that there is no autocorrelation in the series up to a certain number of lags ($H_0: \rho_1 = \rho_2 = \ldots = \rho_{21} = 0$), where $\rho$ represents autocorrelation at different lags.
	\item The alternative hypothesis, which suggests that there is some autocorrelation in the series at least at one lag ($H_1: \rho_i \neq 0$ for some $i \in \{1, 2, \ldots, 21 \}$).
\end{itemize}

The decision rule is to reject $H_0$ if the p-value is less than the significance level $\alpha$ (e.g., 0.05).

For the returns, the test statistic obtained is 27.06 with a p-value of 0.17.

Since the p-value is greater than all conventional significance levels, we do not reject the null hypothesis. 
Thus, there is no significant evidence of autocorrelation in the returns of the series.

For squared returns, the test statistic obtained is 25.61 with a p-value of 0.22.

Similarly, since the p-value is greater than 0.05, we do not reject the null hypothesis for squared returns  at all conventional significance levels either.
This indicates no significant evidence of autocorrelation in the volatility (squared returns) of the series.

\subparagraph{SACF SPACF plots}

The SACF plot measures the correlation between different points in the time series separated by various lags. 
Whilst most autocorrelations are within the confidence intervals, a significant negative correlation at lag 13 suggests there is a season pattern that repeats every 13 periods, so if a series is above average at one point, it tends to be below average 13 periods later and vice versa. 
Furthermore, a lag on the border of significance at 20 suggests a possible longer cynical effect, although this is not as pronounced.
This is also evident for lag 9 since it is just below significance in the SACF but is significant in the SPACF, indicating a direct negative influence from the observation 9 periods ago on the current observation, after accounting for the influences of all observations in between. 
In summary, there is no consistent pattern of significant lags, which would typically be used to identify AR or MA components. 
The presence of significant lags informs us the time series is not white noise and exhibits autocorrelation. 

\subparagraph{Daily Log-returns}

The plot indicates considerable fluctuation around the mean of \amu, whilst the returns do not display a clear trend or seasonal pattern. 
The volatility appears to be clustered in certain periods, indicative of heteroskedacity where periods of high volatility are followed by high volatility and vice versa. 

\subsection*{Question b}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\rowcolor{gray!50}
\multicolumn{3}{|c|}{GARCH} & \multicolumn{3}{|c|}{GJR-GARCH} \\ 
\hline
Parameter & Estimate & P-value & Parameter & Estimate & P-value \\ 
\hline
$\omega$ & \bw & \bpi & $\omega$ & \bwii & \bpgi \\
\hline
$\alpha$ & \ba & \bpii & $\alpha$ & \baii & \bpgii \\
\hline
 $\beta$ & \bb & \bpiii & $\beta$ & \bbii & \bpgiii \\
\hline
GARCH-t & & & $\gamma$ & \bgii & \bpgiv \\
\hline
 $\omega$& \bwi & \bpti & GJR-GARCH-t &  & \\
\hline
 $\alpha$& \bai & \bptii & $\omega$ & \bwiii & \bpgtp \\
\hline
 $\beta$ & \bbi & \bptiii & $\alpha$ & \baiii & \bpgtpi \\
\hline
$\nu$ & \bvi & \bptiv & $\beta$ & \bbiii & \bpgtpii \\
\hline
& & & $\gamma$ & \bgiii & \bpgtpiii \\
\hline
& & & $\nu$ & \bviii & \bpgtpiv \\
\hline
\end{tabular}
\end{table}


\subsubsection*{Interpretation:}

Where $\omega$ is the constant term of the model, representing the long run average variance when all other terms are zero, 
 $\alpha$ is the coefficient representing the contribution of past squared innovations (lagged error terms) to the current variance, indicating how much past volatility affects current volatility. 
  $\beta$ is the coefficient representing the contribution of past conditional variance to the current variance, capturing the persistence of volatility shocks. 
  $\gamma$ is the coefficient specific to GJR-GARCH models, capturing the asymmetric effect of negative shocks (leverage effect), where negative shocks have a different impact on volatility than positive shocks of the same magnitude. 
  $\nu$ is the degrees of freedom parameter in the t-distribution and is related to the kurtosis of the distribution, with lower values indicating heavier tails. 

For the GARCH model with a normal distribution, the estimates for $\alpha$ and $\beta$ are \bw and \bb, respectively with p values indicating that only $\alpha$ is statistically significant at a conventional level ($p < 0.05$). Thus the model suggests that past shocks have a significant impact on current volatility, but the effect is not persistent. 

For the GARCH-t model, $\beta$ is signifiant, indicating persistence in volatility and $\nu$ is also significant, suggesting that the distribution of innovations has heavier tails than the normal distribution. 
Thus, the presence of heavy tails in the data is signifiant, which could be important fore forecasting \ldots

For the GJR-GARCH model, $\omega$ is significant, but $\alpha$ and $\beta$ are very small with correspondingly very large p-values. 
Thus, negative shocks might have a different impact on volatility, although this effect is not statistically significant at the 5\% level

For the GJR-GARCH-t model, both $\alpha$ and $\beta$ are significant, indicating that past shocks and volatility are important for current volatility, and $\nu$ is significant, indicating heavy tails. 
Although, $\gamma$ is not significant, suggesting the asymmetric effects of shocks is not statistically significant. 
Thus, both past shocks and heavy tails are significant in modelling volatility but asymmetric effects of shocks are not significant. 

Overall, a garch-t or GJR-GARCH-t model might be preferred


\subsection*{Question c}
\subsubsection*{Plot of NIC}
 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{../../docs/figures/NIC.png}
    \caption{News Impact Curve Plot}
    \label{fig:logreturn}
\end{figure}

\subsubsection*{Interpretation:}
Drawbacks of the GARCH models are obvious here, since the conditional variance is unable to respond asymmetrically to shocks in $y_{t}$, that is, a positive return has the same effect as a negative return upon variance. Since it is argued that negative innovations to shock returns tend to increase volatility more than positive innovations of the same magnitude. 

This plot offers a visual representation of the effect that new information has on the volatility predicted by different GARCH models. 
The standard GARCH model with normal innovations shows that the impact on conditional variance is symmetrical, both positive and negative shocks of the same magnitude have the same effect on predicted volatility, leaving the leverage effect unaccounted for. 

The GARCH model with a student's t-distribution is also symmetric but flatter compared to the standard GARCH model, indicating less sensitivity to shocks in general, due to the heavier tails in the t-distribution, reducing the impact of outliers. 

The GJR-GARCH model with normal distribution shows a pronounced asymmetry where negative shocks increase the conditional variance more than positive shocks. 
The model accounts for the leverage effect, where bad news influences volatility more than good news. 

The GJR-GARCH model with Student's t-distribution combines the properties of the GJR-GARCH with the heavier tails of the t-distribution. Showing an asymmetrical impact of shocks on volatility, similar to the GJR-GARCH but with the additional effect of heavier tails in the distribution of shocks. 
Heavier tails may imply that extreme values are more probable than in a normal distortion, and the impact curve is less steep for large magnitude shocks, indicating large shocks increase volatility less than what a normal distribution would suggest. 
Overall, both models with Student's t-distribution suggests that the models account for heavy tails in the distribution of shocks, whilst the GJR-GRACH models reveal an asymmetric response to shocks, highlighting the importance of the leverage effect. 

\subsection*{Question d}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{headercolor}
GARCH & Test statistic & P-value \\ 
\hline
Z & \zone & \pone \\ 
\hline
\(Z^2\) & \zfive & \pfive \\ 
\hline
\rowcolor{headercolor}
GARCH-t & Test statistic & P-value \\ 
\hline
Z & \ztwo & \ptwo \\ 
\hline
\(Z^2\) & \zsix & \psix \\ 
\hline
\end{tabular}
\quad % To add some spacing between the two tables
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{headercolor}
GJR-GARCH & Test statistic & P-value \\ 
\hline
Z & \zthree & \pthree \\ 
\hline
\(Z^2\) &  \zseven & \pseven \\ 
\hline
\rowcolor{headercolor}
GJR-GARCH-t & Test statistic & P-value \\ 
\hline
Z & \zfour & \pfour \\ 
\hline
\(Z^2\) & \zeight & \peight \\ 
\hline
\end{tabular}
\end{table}

\subsubsection*{Interpretation:}

High p-values for the residuals indicate there is no statistical evidence to reject the null hypothesis that the residuals follow the respective distributions. 
Suggesting the residuals are white noise, meaning they are normally distributed with no autocorrelation. 

For the squared residuals, the p-values are high but slightly less so, again suggesting that there is no statistical evidence to reject the null hypothesis of no autocorrelation in the squared residuals. Indicating there is no ARCH effect and the conditional variance is well captured by the model. 

Whilst the LBQ p-values for the residuals are very high across all models, suggesting that none of the models leaves unexplained autocorrelation in the returns, which means that all models are adequate in this respect. 

The p-values for the squared residuals, which help to identify volatility clustering or ARCH effects, are also high across all models. 
However, in this context, the model with the highest p-value (corresponding to lowest test stat) for the square residuals is the GARCH-t model, indicating the least amount of autocorrelation and possibly the best fit among the compared models. 

\subsection*{Question e}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\rowcolor{gray!50}
& GARCH & GARCH-t & GJR-GARCH & GJR-GARCH-t \\
\hline
RMSFE & \rmsfei & \rmsfeii & \rmsfeiii & \rmsfeiv \\
\hline
DM Test statistic & NA & \dm & \dmi & \dmii \\
\hline
P-value & NA & \dmpii & \dmpiv & \dmpv \\
\hline
\end{tabular}
\end{table}

\subsubsection*{Interpretation:}

The RMSFE values indicate that GARCH+ has the smallest forecast error at 7.29, followed by GJR-GARCH-t at 7.30, GARCH at 7.32, and GJR-GARCH at 7.32. Lower RMSFE values suggest better forecast accuracy.

Diebold-Mariano (DM) Test results are only meaningful for GARCH+ and GJR-GARCH-t since GARCH is the benchmark. For GARCH+, the DM Test statistic is 2.00 with a p-value of 0.98, and for GJR-GARCH-t, the statistic is 1.83 with a p-value of 0.97. Since the p-values are much higher than the typical significance levels (e.g., 0.05 or 0.10), there's no statistical evidence that the forecast accuracy of GARCH+ and GJR-GARCH-t is different from GARCH.

Thus, although GARCH+ has a slightly lower RMSFE, the Diebold-Mariano test does not confirm its superiority over the benchmark GARCH model in terms of predictive accuracy. Economically, this suggests that there might be no practical benefit from using more complex models over the simpler GARCH model for forecasting this particular variance, as they do not provide statistically significant improvements in forecast accuracy.

\subsection*{Question f}
\subsubsection*{Interpretation:}

The choice of GARCH model involves a tradeoff between model misspecification and estimation noise. On one hand, simpler models like the standard GARCH have fewer parameters and are less prone to estimation noise, but may suffer from misspecification if the true volatility process exhibits features that the model cannot capture (e.g., asymmetry, time-varying volatility).

More flexible models like the GJR-GARCH or GARCH with additional components (GARCH+) can potentially better fit the data by accounting for these complexities, reducing misspecification. However, they require estimating more parameters, introducing greater estimation noise.

While complex models offer an improved in-sample fit, they do not necessarily outperform simpler models in out-of-sample forecasting due to the increased estimation noise. The benefits of reduced misspecification may be offset by the noise from estimating additional parameters, especially in small samples.

Ultimately, the optimal model balances the competing goals of parsimony and flexibility. Simple models are preferred when estimation noise is a greater concern, while more complex specifications are justified if the true data-generating process exhibits features that warrant additional modeling components, even at the cost of some estimation noise.

Model selection criteria like information criteria aim to navigate this tradeoff by penalizing overly complex models. However, the final choice depends on the relative importance of in-sample fit versus out-of-sample forecasting performance for the specific application.

\newpage
\section*{Appendix}

For reproduction of said script, see \url{https://github.com/oddish3/FE-CW/tree/master}

\lstinputlisting[language=R]{../../code/Script.R}




\end{document}

